{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71717ac4-93fb-421c-b8b4-db60ce6570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b525f15c-a0b8-4c2e-ade3-d3a2b8949be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_metric_learning import miners, losses, distances \n",
    "import os\n",
    "from transformers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import random \n",
    "import math \n",
    "import faiss # pip install faiss-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3fe1cc-2b52-4354-808f-8423cdd03bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886a02c229a641c9adce50844fd30982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"law_candidates_60K.jsonl\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "queries, passages, answers = [], [], []\n",
    "for i in tqdm(range(len(data)), position=0, leave=True):\n",
    "    query = data[i][\"summary\"]\n",
    "    passage = data[i][\"text\"]\n",
    "    answer = data[i][\"answer\"]\n",
    "    queries.append(query)\n",
    "    passages.append(passage)\n",
    "    answers.append(answer)\n",
    "\n",
    "all_data = pd.DataFrame(list(zip(queries, passages, answers)), columns=[\"queries\", \"passages\", \"answers\"])\n",
    "\n",
    "model_name = \"monologg/kobigbird-bert-base\"\n",
    "q_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "p_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952d0619-a175-44e2-8492-74e8a8b7c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index_dict = {} \n",
    "\n",
    "for i in range(len(queries)):\n",
    "    query_index_dict[queries[i]] = [] \n",
    "\n",
    "for i in range(len(queries)): \n",
    "    query_index_dict[queries[i]].append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a13986-494e-4ebc-9cde-537a0460be7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee81fb4852eb4f90b04194eaecf6d103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60069 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([60069, 512]), torch.Size([60069, 512]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = all_data[\"passages\"].values \n",
    "\n",
    "all_context_input_ids, all_context_attn_masks = [], [] \n",
    "for i in tqdm(range(len(candidates)), position=0, leave=True): \n",
    "    encoded_inputs = p_tokenizer(str(candidates[i]), max_length=512, truncation=True, padding=\"max_length\") \n",
    "    all_context_input_ids.append(encoded_inputs[\"input_ids\"]) \n",
    "    all_context_attn_masks.append(encoded_inputs[\"attention_mask\"])  \n",
    "    \n",
    "all_context_input_ids = torch.tensor(all_context_input_ids, dtype=int) \n",
    "all_context_attn_masks = torch.tensor(all_context_attn_masks, dtype=int) \n",
    "\n",
    "all_context_input_ids.shape, all_context_attn_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0abbda9-589a-4de3-8188-12e7407bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairData(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super(PairData, self).__init__()\n",
    "        self.data = df\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index]\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "class custom_collate(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\")\n",
    "        self.chunk_size = 512\n",
    "    def __call__(self, batch):\n",
    "        q_input_ids, q_attn_masks, q_labels = [], [], []\n",
    "        p_input_ids, p_attn_masks, p_labels = [], [], [] \n",
    "        ids = 0\n",
    "        all_queries = [] \n",
    "        for idx, row in enumerate(batch):\n",
    "            query, passage = row[0], row[1] \n",
    "            encoded_q = self.tokenizer(query, max_length=self.chunk_size, padding=\"max_length\", truncation=True, return_tensors=\"pt\") \n",
    "            encoded_p = self.tokenizer(passage, max_length=self.chunk_size, padding=\"max_length\", truncation=True, return_tensors=\"pt\") \n",
    "            q_input_ids.append(encoded_q[\"input_ids\"]) \n",
    "            q_attn_masks.append(encoded_q[\"attention_mask\"]) \n",
    "            q_labels.append(ids) \n",
    "            \n",
    "            p_input_ids.append(encoded_p[\"input_ids\"]) \n",
    "            p_attn_masks.append(encoded_p[\"attention_mask\"]) \n",
    "            p_labels.append(ids) \n",
    "            ids += 1 \n",
    "        q_input_ids = torch.stack(q_input_ids, dim=0).squeeze(dim=1)  \n",
    "        q_attn_masks = torch.stack(q_attn_masks, dim=0).squeeze(dim=1) \n",
    "        q_labels = torch.tensor(q_labels, dtype=int)  \n",
    "        \n",
    "        p_input_ids = torch.stack(p_input_ids, dim=0).squeeze(dim=1) \n",
    "        p_attn_masks = torch.stack(p_attn_masks, dim=0).squeeze(dim=1) \n",
    "        p_labels = torch.tensor(p_labels, dtype=int) \n",
    "        \n",
    "        return q_input_ids, q_attn_masks, q_labels, p_input_ids, p_attn_masks, p_labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbb9486-2b85-4f25-ad8b-7868d332e0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/00ac7c2886f9d4555133877badce522b93b38439d90b0135d9b414cc1fafd167.34d17d2d06e0d29acc69761e3ddeced0dfdcf4cefa0aa81a1bb267a7dfdd5bcb\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e2eb4ad30139b806997f999b45c0a0d9ea38b14e0d97f42db852be137e061b1e.616843352d77fff459e989408eaacf1280dc39dcd346ff746aa3b3fbe6a123d9\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9bea998b48658e35dd618115a266f6c173183a9a4261fc6e40730d74c4b67899.e3640e465e51ce85d94923a0b396029ecc2e3e4c7764031eee57ab272637652d\n",
      "loading file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e2d347346e3e13ea8d3e50ab2baef2cde9e7942cb05158cbd0effaa54af4e6e0.0763f758d6f4d3780831e69ac1702755a04a05bdbcb3e4f7692b6b546171ccb1\n"
     ]
    }
   ],
   "source": [
    "train_set = PairData(train_df) \n",
    "valid_set = PairData(valid_df) \n",
    "collate = custom_collate() \n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=16, collate_fn=collate, shuffle=True) \n",
    "valid_dataloader = DataLoader(valid_set, batch_size=16, collate_fn=collate, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941ddbf2-e1ea-4313-b46d-8a8d9b3ef019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3ff1f36a44e63a0ac32fcc55ff4c268a360e07ee22869bbc20ded21da8fdd596.4449f16b91f50859dc03ca5c81261c9952b3176fd389a7e99d067b33c0a8f3a1\n",
      "Model config BigBirdConfig {\n",
      "  \"_name_or_path\": \"monologg/kobigbird-bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 3,\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32500\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/kobigbird-bert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/961f8c88dfa85a4b5bcdadb16abba817cf4cb6bf38ad0e2114249f4429efe451.d0cebe466881f586582c73d04be9d48ce3aafa4a491ec9898f4ea4b9e010ad41\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "q_encoder = AutoModel.from_pretrained(model_name) \n",
    "q_encoder.to(device) \n",
    "\n",
    "p_encoder = AutoModel.from_pretrained(model_name) \n",
    "p_encoder.to(device)  \n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7653d97-082f-4bda-be6b-ffb4ec53bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdabb1817e54b378911b7fd7314d767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461c32ca594d4a8eac333b0ee194b66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3004 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b9f204d98e434b98ca74c51c2db6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train loss : 0.006242730487361253 | avg valid loss : 0.05154453043607955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7a3d295a4f45dbb3767c2ac85e88e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate embeddings shape: (6400, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05afdd78d2c47edafeda3f9d5802377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Recall:   0%|          | 0/6007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.04891515454192331 | Recall@5: 0.07004605737750404\n",
      "saved current best checkpoint!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbac4cecf144543a357b39562ad21e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3004 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m full_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((q_labels, p_labels))  \n\u001b[1;32m     34\u001b[0m shuffled_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(full_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m---> 35\u001b[0m full_embeddings, full_labels \u001b[38;5;241m=\u001b[39m \u001b[43mfull_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshuffled_idx\u001b[49m\u001b[43m]\u001b[49m, full_labels[shuffled_idx] \n\u001b[1;32m     37\u001b[0m hard_pairs \u001b[38;5;241m=\u001b[39m miner(full_embeddings, full_labels) \n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(full_embeddings, full_labels, hard_pairs) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate both validation loss as well as the recall score \n",
    "epochs = 30\n",
    "params = list(q_encoder.parameters()) + list(p_encoder.parameters()) \n",
    "optimizer = AdamW(params, lr=2e-5, eps=1e-8) \n",
    "t_total = len(train_dataloader) * epochs \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.05*t_total), num_training_steps=t_total) \n",
    "\n",
    "miner = miners.MultiSimilarityMiner() \n",
    "loss_func = losses.MultiSimilarityLoss()\n",
    "\n",
    "q_encoder.zero_grad() \n",
    "p_encoder.zero_grad() \n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "best_recall = 0\n",
    "\n",
    "train_losses, val_losses = [], [] \n",
    "val_recall_1, val_recall_5 = [], [] \n",
    "\n",
    "for epoch_i in tqdm(range(0, epochs), desc=\"Epochs\", position=0, leave=True, total=epochs):\n",
    "    train_loss = 0 \n",
    "    q_encoder.train() \n",
    "    p_encoder.train() \n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch: \n",
    "        for step, batch in enumerate(tepoch): \n",
    "            if step == 30:\n",
    "                break \n",
    "            batch = tuple(t.to(device) for t in batch) \n",
    "            q_input_ids, q_attn_masks, q_labels, p_input_ids, p_attn_masks, p_labels = batch \n",
    "            q_embeddings = q_encoder(q_input_ids, q_attn_masks).pooler_output \n",
    "            p_embeddings = p_encoder(p_input_ids, p_attn_masks).pooler_output \n",
    "            full_embeddings = torch.cat((q_embeddings, p_embeddings), dim=0) \n",
    "            full_labels = torch.cat((q_labels, p_labels))  \n",
    "            shuffled_idx = torch.randperm(full_embeddings.shape[0]) \n",
    "            full_embeddings, full_labels = full_embeddings[shuffled_idx], full_labels[shuffled_idx] \n",
    "            \n",
    "            hard_pairs = miner(full_embeddings, full_labels) \n",
    "            loss = loss_func(full_embeddings, full_labels, hard_pairs) \n",
    "            train_loss += loss.item() \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            optimizer.step() \n",
    "            scheduler.step() \n",
    "            q_encoder.zero_grad() \n",
    "            p_encoder.zero_grad() \n",
    "            tepoch.set_postfix(loss=train_loss / (step+1)) \n",
    "            time.sleep(0.1) \n",
    "    avg_train_loss = train_loss / len(train_dataloader) \n",
    "    \n",
    "    val_loss = 0 \n",
    "    q_encoder.eval() \n",
    "    p_encoder.eval() \n",
    "    for step, batch in tqdm(enumerate(valid_dataloader), position=0, leave=True, total=len(valid_dataloader)): \n",
    "        if step == 30:\n",
    "            break \n",
    "        batch = tuple(t.to(device) for t in batch) \n",
    "        q_input_ids, q_attn_masks, q_labels, p_input_ids, p_attn_masks, p_labels = batch \n",
    "        with torch.no_grad(): \n",
    "            q_embeddings = q_encoder(q_input_ids, q_attn_masks).pooler_output\n",
    "            p_embeddings = p_encoder(p_input_ids, p_attn_masks).pooler_output \n",
    "            full_embeddings = torch.cat((q_embeddings, p_embeddings), dim=0) \n",
    "            full_labels = torch.cat((q_labels, p_labels)) \n",
    "            shuffled_idx = torch.randperm(full_embeddings.shape[0]) \n",
    "            full_embeddings, full_labels = full_embeddings[shuffled_idx], full_labels[shuffled_idx] \n",
    "            loss = loss_func(full_embeddings, full_labels) \n",
    "            val_loss += loss.item()  \n",
    "    avg_val_loss = val_loss / len(valid_dataloader) \n",
    "    print(f\"avg train loss : {avg_train_loss} | avg valid loss : {avg_val_loss}\") \n",
    "    train_losses.append(avg_train_loss) \n",
    "    val_losses.append(avg_val_loss) \n",
    "    \n",
    "    \n",
    "    # calculating Recall \n",
    "    with torch.no_grad(): \n",
    "        recall = 0 \n",
    "        p_encoder.eval() \n",
    "        p_embs = [] \n",
    "        inference_dataset = TensorDataset(all_context_input_ids, all_context_attn_masks) \n",
    "        inference_sampler = SequentialSampler(inference_dataset) \n",
    "        inference_dataloader = DataLoader(inference_dataset, sampler=inference_sampler, batch_size=64) \n",
    "        for step, batch in tqdm(enumerate(inference_dataloader), position=0, leave=True, total=len(inference_dataloader)): \n",
    "            if step == 100: \n",
    "                break \n",
    "            batch = (t.to(device) for t in batch) \n",
    "            b_input_ids, b_attn_masks = batch \n",
    "            p_emb = p_encoder(b_input_ids, b_attn_masks).pooler_output \n",
    "            for i in range(p_emb.shape[0]): \n",
    "                p_embs.append(torch.reshape(p_emb[i], (-1, 768)))\n",
    "        p_embs = torch.cat(p_embs, dim=0) \n",
    "        p_embs = p_embs.detach().cpu().numpy() \n",
    "        print(f\"candidate embeddings shape: {p_embs.shape}\") \n",
    "        # use faiss cosine similarity \n",
    "        index = faiss.IndexIDMap2(faiss.IndexFlatIP(768)) \n",
    "        faiss.normalize_L2(p_embs.astype(np.float32)) \n",
    "        index.add_with_ids(p_embs.astype(np.float32), np.array(range(0, len(p_embs)), dtype=int)) \n",
    "        index.nprobe = 64 \n",
    "\n",
    "        top_1 = 0\n",
    "        top_5 = 0 \n",
    "        q_encoder.eval() \n",
    "        val_questions = valid_df[\"queries\"].values  \n",
    "        for sample_idx in tqdm(range(len(val_questions)), position=0, leave=True, desc=\"Calculating Recall\"):\n",
    "            query = val_questions[sample_idx] \n",
    "            encoded_query = q_tokenizer(str(query), max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device) \n",
    "            q_emb = q_encoder(**encoded_query).pooler_output \n",
    "            q_emb = q_emb.detach().cpu().numpy() \n",
    "            distances, indices = index.search(q_emb, 1000) \n",
    "\n",
    "            correct_idx = query_index_dict[query] \n",
    "            cnt = 0 \n",
    "            for idx in correct_idx: \n",
    "                if idx in indices[0][:1]: \n",
    "                    cnt += 1 \n",
    "            top_1 += cnt / len(correct_idx)  \n",
    "\n",
    "            cnt = 0 \n",
    "            for idx in correct_idx:\n",
    "                if idx in indices[0][:5]:\n",
    "                    cnt += 1 \n",
    "            top_5 += cnt / len(correct_idx) \n",
    "\n",
    "        avg_top_1 = top_1 / len(val_questions) \n",
    "        avg_top_5 = top_5 / len(val_questions) \n",
    "\n",
    "        print(f\"Recall@1: {avg_top_1} | Recall@5: {avg_top_5}\") \n",
    "        val_recall_1.append(avg_top_1) \n",
    "        val_recall_5.append(avg_top_5) \n",
    "\n",
    "        if avg_top_5 > best_recall: \n",
    "            best_recall = avg_top_5 \n",
    "            torch.save(q_encoder.state_dict(), f\"query_encoder_law_QA.pt\") \n",
    "            torch.save(p_encoder.state_dict(), f\"passage_encoder_law_QA.pt\") \n",
    "            print(\"saved current best checkpoint!\") \n",
    "            \n",
    "with open(\"train_losses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_losses, f)\n",
    "    \n",
    "with open(\"val_losses.pkl\", \"wb\") as f: \n",
    "    pickle.dump(val_losses, f) \n",
    "    \n",
    "with open(\"val_recall_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(val_recall_1, f) \n",
    "\n",
    "with open(\"val_recall_5.pkl\", \"wb\") as f: \n",
    "    pickle.dump(val_recall_5, f) \n",
    "    \n",
    "print(\"done training!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15398a-d20c-4118-96b3-8d83f2d187a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9676a-d450-411f-9c15-043e4952fecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207689f-0b04-4a82-a24b-48d5a6c25958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
